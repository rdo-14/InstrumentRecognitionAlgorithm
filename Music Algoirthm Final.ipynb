{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the instruments to labels to classify\n",
    "instrument_to_label = {\n",
    "    \"violin\": 0,\n",
    "    \"sax\": 1,\n",
    "    \"trumpet\": 2,\n",
    "    \"piano\": 3,\n",
    "    \"organ\": 4,\n",
    "    \"flute\": 5,\n",
    "    \"clarinet\": 6\n",
    "}\n",
    "\n",
    "# defining base path to the project folders including testing and training directories \n",
    "# when running this make sure to change location of base path and have a Training and Testing Folder\n",
    "base_path = os.path.dirname(os.path.abspath(r\"C:\\Users\\ryan3\\Final Project Machine Learning\\Training\"))  #current path of folder\n",
    "train_path = os.path.join(base_path, \"Training\")\n",
    "test_path = os.path.join(base_path, \"Testing\")\n",
    "\n",
    "# checking if the folder pathing is same as mine (will provide picture) \n",
    "if not os.path.isdir(train_path) or not os.path.isdir(test_path):\n",
    "    raise FileNotFoundError(f\"Expected 'Training' and 'Testing' folders in {base_path}.\")\n",
    "\n",
    "X_train = [] # store speectrograms and labels\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bb6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 525 spectrograms and 525 labels.\n"
     ]
    }
   ],
   "source": [
    "# loop through each instrument and its label\n",
    "for instrument, label in instrument_to_label.items():\n",
    "    instrument_folder = os.path.join(train_path, instrument) # instrument path\n",
    "    if os.path.isdir(instrument_folder):  # check folder\n",
    "        for file in os.listdir(instrument_folder):\n",
    "            if file.endswith(\".wav\"):  # checking for correct files\n",
    "                file_path = os.path.join(instrument_folder, file)\n",
    "                try:\n",
    "                    # lodaing audio files\n",
    "                    y, sr = librosa.load(file_path, sr=16000)\n",
    "                    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "                    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                    X_train.append(mel_spec_db) # adds spectrogram\n",
    "                    y_train.append(label) # adds labels\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# change lists into arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Processed {len(X_train)} spectrograms and {len(y_train)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data tensor shape: torch.Size([420, 1, 128, 94])\n",
      "Validation data tensor shape: torch.Size([105, 1, 128, 94])\n"
     ]
    }
   ],
   "source": [
    "# normalizing data so mean = 0 and standard deviation is 1 for all samples\n",
    "X_train = (X_train - np.mean(X_train, axis=(1, 2), keepdims=True)) / np.std(X_train, axis=(1, 2), keepdims=True)\n",
    "\n",
    "max_time_steps = 94\n",
    "X_train = np.array([librosa.util.fix_length(x, size=max_time_steps, axis=1) for x in X_train])\n",
    "\n",
    "# training/validating datasets split \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
    "\n",
    "# converting to pyTorch sensors https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "# Tensors are a specialized data structure that are very similar to arrays and matrices. \n",
    "# In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the modelâ€™s parameters.\n",
    "# in this case coverting spectrograms and labels into tensors \n",
    "# helps with uneven dimensions\n",
    "X_train_tensor = torch.tensor(X_train).unsqueeze(1).float()\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "X_val_tensor = torch.tensor(X_val).unsqueeze(1).float()\n",
    "y_val_tensor = torch.tensor(y_val).long()\n",
    "\n",
    "# dataloadsers creation\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Training data tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"Validation data tensor shape: {X_val_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for classification of instruments\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__() \n",
    "        # 3 convulitonal layers \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) # first layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # second layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # third layer\n",
    "        self.pool = nn.MaxPool2d(2) # correcting the dimensions\n",
    "        self.dropout = nn.Dropout(0.5) # regularizaton \n",
    "        self.fc1 = None  \n",
    "        self.fc2 = nn.Linear(256, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolutional and pooling layers being passed through\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv3(x)))\n",
    "\n",
    "        if self.fc1 is None:  # Dynamically initialize the first fully connected layer based on input size\n",
    "            flattened_size = x.numel() // x.size(0)  # Total elements per sample\n",
    "            self.fc1 = nn.Linear(flattened_size, 256).to(x.device)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flattening the input\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbd492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.9552, Validation Loss: 1.9393, Accuracy: 0.1429\n",
      "Epoch [2/20], Loss: 1.9453, Validation Loss: 1.9400, Accuracy: 0.1333\n",
      "Epoch [3/20], Loss: 1.9361, Validation Loss: 1.9280, Accuracy: 0.2571\n",
      "Epoch [4/20], Loss: 1.9207, Validation Loss: 1.8907, Accuracy: 0.3048\n",
      "Epoch [5/20], Loss: 1.8843, Validation Loss: 1.8744, Accuracy: 0.2476\n",
      "Epoch [6/20], Loss: 1.8506, Validation Loss: 1.8191, Accuracy: 0.3524\n",
      "Epoch [7/20], Loss: 1.7620, Validation Loss: 1.7548, Accuracy: 0.3905\n",
      "Epoch [8/20], Loss: 1.7293, Validation Loss: 1.7005, Accuracy: 0.3619\n",
      "Epoch [9/20], Loss: 1.6884, Validation Loss: 1.6963, Accuracy: 0.3810\n",
      "Epoch [10/20], Loss: 1.6563, Validation Loss: 1.6586, Accuracy: 0.4667\n",
      "Epoch [11/20], Loss: 1.5854, Validation Loss: 1.6474, Accuracy: 0.3714\n",
      "Epoch [12/20], Loss: 1.5333, Validation Loss: 1.6587, Accuracy: 0.4000\n",
      "Epoch [13/20], Loss: 1.4332, Validation Loss: 1.6356, Accuracy: 0.3810\n",
      "Epoch [14/20], Loss: 1.4496, Validation Loss: 1.5892, Accuracy: 0.4000\n",
      "Epoch [15/20], Loss: 1.4424, Validation Loss: 1.5610, Accuracy: 0.4381\n",
      "Epoch [16/20], Loss: 1.4118, Validation Loss: 1.5472, Accuracy: 0.4381\n",
      "Epoch [17/20], Loss: 1.3384, Validation Loss: 1.5036, Accuracy: 0.4667\n",
      "Epoch [18/20], Loss: 1.2882, Validation Loss: 1.5080, Accuracy: 0.4857\n",
      "Epoch [19/20], Loss: 1.2049, Validation Loss: 1.5154, Accuracy: 0.5143\n",
      "Epoch [20/20], Loss: 1.2470, Validation Loss: 1.5075, Accuracy: 0.4571\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# setting device to GPU is is able to\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(num_classes=len(instrument_to_label)).to(device)\n",
    "\n",
    "# Compute class weights for imbalanced datasets\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# defining loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad() # clear graidents\n",
    "        outputs = model(X_batch) # forward passing\n",
    "        loss = criterion(outputs, y_batch) # compute loss\n",
    "        loss.backward() # backward passing\n",
    "        optimizer.step() # update model weights\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval() \n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_loss += criterion(val_outputs, y_val_batch).item()\n",
    "            correct += (val_outputs.argmax(1) == y_val_batch).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / len(y_val)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # had issues with occasional stoppages here as a backup\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c7182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84513d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted instrument: piano\n"
     ]
    }
   ],
   "source": [
    "test_file = os.path.join(test_path, \"Lets go dodgers!.wav\") # only need audio file name here make sure it is .wav format\n",
    "y, sr = librosa.load(test_file, sr=16000)\n",
    "mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "# normalizing and resizing  the test data\n",
    "mel_spec_db = (mel_spec_db - np.mean(mel_spec_db)) / np.std(mel_spec_db)\n",
    "mel_spec_db = librosa.util.fix_length(mel_spec_db, size=max_time_steps, axis=1)\n",
    "X_test = torch.tensor(mel_spec_db).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(X_test)\n",
    "    predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "# predicted class index mapped back to instrument name\n",
    "label_to_instrument = {v: k for k, v in instrument_to_label.items()}\n",
    "predicted_instrument = label_to_instrument.get(predicted_class, \"Unknown\")\n",
    "print(f\"Predicted instrument: {predicted_instrument}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0ece7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249db11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
